---
title: "ML_Problem_Proposal_Wine"
author: "Shivam Patel, Ethan Gruis, Ben Siglow"
date: "10/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
##install.packages("SHAPforxgboost")
#install_github("AppliedDataSciencePartners/xgboostExplainer")
library(devtools)
library(xgboostExplainer)
library(tidyverse)
library(ggdark)
library(rattle)	
library(RColorBrewer)
library(randomForest)
library(caTools)
library(caret)
library(rpart)
library(splitstackshape)
library(xgboost)
library(Metrics)
library(pROC)
library(SHAPforxgboost)
library(xgboostExplainer)
```


# Load in the data
```{r}
link <- 'https://www.dropbox.com/s/mjj5x2n2wfjxqs9/BordeauxWines.csv?dl=1'
wine_data <- read_csv(link, locale = readr::locale(encoding = "latin1"))

# summary(wine_data)
# str(wine_data)
```



# Mutate our variables into factors & Remove variables with constant values
```{r}
wine_cols <- c(5:989)
wine_data[,wine_cols] <- lapply(wine_data[,wine_cols], factor)

# ISSUE: when factorized, wine sometimes has columns with only 1 factor, this selects only columns with multiple factors and drops the rest
wine_fixed <- wine_data[, sapply(wine_data, function(col) length(unique(col))) > 1]
```

# Remove dollar signs from Price.
```{r}
# Fix Price Variable
wine_fixed$Price <- str_replace(wine_fixed$Price, "\\$", "")

wine_fixed$Price <- as.numeric(wine_fixed$Price)

wine_fixed <- drop_na(wine_fixed)

# Fix ColNames
colnames(wine_fixed) <- make.names(names(wine_fixed))

# For some reason there are two WELL DONE columns, so lets change the name of one of them?
colnames(wine_fixed)[489] <- 'WELL.DONE.2'

# Notes:
# what does well done mean in terms of wine???
# look at price - some are in $/ML
```

```{r}
#wine_fixed$Year
#not_price <- wine_fixed[c(2,3,5:620)]
#wine_fixed$Price

#rfImpute(wine_fixed$Price, wine_fixed$Year)
```


# Visualizations
```{r}
price_score_plot <- ggplot(wine_fixed, aes(x = Price, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

price_score_plot

score_year_plot <- ggplot(wine_fixed, aes(x = Year, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

score_year_plot

hist_score_plot <- ggplot(wine_fixed, aes(x = Score)) + 
  geom_histogram(binwidth = 1, color="black", fill="white") +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  xlim(70, 100) +
  dark_theme_gray()

hist_score_plot
```

# Linear regressions
```{r}
# LM 1 = Score predicted by Price
lm_1 <- lm(Score ~ Price, data = wine_fixed)

summary(lm_1)
```

```{r}
# LM 2 = Score predicted by Year
lm_2 <- lm(Score ~ Year, data = wine_fixed)

summary(lm_2)
```

# Decision Trees
```{r}
tree_1 <- rpart(Score ~., # Need to run tree on train and test set Ethan
                data = wine_fixed)

plotcp(tree_1)
summary(tree_1)# Plot cp
```


### Uncomment and move above 
##Strat split to the train/test. BS
# ```{r}
# 
# set.seed(1984)
# split <-  stratified(wine_fixed, c('Score'), .80,bothSets = TRUE)
# train_strat <- split[[1]]
# test_strat <- split[[2]]
# ```


# Random Forest Model Results 
```{r}
load("./random_forest_model.rda")
rf_mod <- fit

rf_mod ## Etahn hard code square root for rmse 
```


## Created XG frame and used lapply to convert to numeric. BS
```{r}
wine_XG <- wine_fixed

wine_XG[,c(2,4:620)] <-  lapply(wine_XG[,c(2,4:620)], as.numeric)
```


## Re Strat split on new set of numeric Data. BS
```{r}
set.seed(1984)
split <-  stratified(wine_XG, c('Score'), .80,bothSets = TRUE)
train_strat <- split[[1]]
test_strat <- split[[2]]

```

- 
### First XG Run. BS
### Converting to xgb magic. BS
```{r}
wtrain <- xgb.DMatrix(data = as.matrix(train_strat[,c(2,4:620)]), label = as.numeric(train_strat$Score))
# Create test matrix
wtest <- xgb.DMatrix(data = as.matrix(test_strat[, c(2, 4:620)]), label = as.numeric(test_strat$Score)) 
```


## Training basic XG. BS
```{r}
set.seed(1984)
winebeast <- xgboost(data = wtrain, 
               
               nrounds = 1000, 
               nthread = 3, 
               
               verbose = 1, 
                print_every_n = 20, )

```

## Testing basic XG. BS
```{r}
wine_preds <- predict(winebeast, wtrain) 
# Join predictions and actual
wine_dat <- cbind.data.frame(wine_preds , train_strat$Score)
names(wine_dat) <- c("predictions", "response")


wine_preds_1 <- predict(winebeast, wtest)


```
## Check Accuracy. BS
```{r}
##install.packages("Metrics")
## Based on this remedeal mean square we murdered this. 
actual <- test_strat$Score
predicted <- wine_preds_1
rmse(actual, predicted)
```
## Plotting results to first XG-boost.
```{r}
# Martin graph code 
plot_data <- cbind.data.frame(actual, predicted)
plot_data$col <- log(abs(actual - predicted) + 1)
winebeast_plot <- ggplot(plot_data[actual > 60,], aes(x = actual, y = predicted, color = col)) + ## removing outlier score of 60 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nactual score")
winebeast_plot
```


## Check number of itterations in model. BS
```{r}
set.seed(1984)
wine_amount <- xgb.cv(data = wtrain, 
              
              nfold = 5, 
               
               eta = 0.1, 
              
               nrounds = 2000, 
               early_stopping_rounds = 50, 
               
               verbose = 1, 
               nthread = 4, 
               print_every_n = 20) 

```
## Best itteration count is 756, 1000 will be a realistic iterations number for final XGBoost. BS
## Work on heat maps.BS
```{r}
# Tuning Loop - Martin Barron contribution. 
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
wcv_params <- expand.grid(max_depth_vals, min_child_weight)
names(wcv_params) <- c("max_depth", "min_child_weight")
# Create results vector
rmse_vec <- rep(NA, nrow(wcv_params)) 
# Loop through results
for(i in 1:nrow(wcv_params)){
  set.seed(1984)
  bst_tune <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = wcv_params$max_depth[i], # Set max depth
              min_child_weight = wcv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no im
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20) # Prints out result every 20th iteration

  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
}

```

```{r}
##wcv_params
##rmse_vec
##res_db
```


## Martin Heat plot of tuning.BS 
```{r}
# More Martin ggplot code 
# Join results in dataset
res_db <- cbind.data.frame(wcv_params, rmse_vec)
names(res_db)[3] <- c("rmse") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print RMSE heatmap
wine_heat <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$rmse), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "RMSE") # Set labels
wine_heat # Generate plot
```

## Need to create multiple XG models to determine the prefered eta.
# .1 eta run. BS
```{r}
set.seed(1984)
wine_eta1 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 3, # Set number of parallel threads
              print_every_n = 20) # Set evaluation metric to use

```
```{r}
wine_eta1
```

# .05 eta run
```{r}
set.seed(1984)
wine_eta2 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 3, # Set number of parallel threads
              print_every_n = 20)
```

# .01 eta run
```{r}
set.seed(1984)
wine_eta3 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 3, # Set number of parallel threads
              print_every_n = 20)
```

# .005 eta run
```{r}
set.seed(1984)
wine_eta4 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 3, # Set number of parallel threads
              print_every_n = 20)
```

#.001 eta run
```{r}
set.seed(1984)
wine_eta5 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.001, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 3, # Set number of parallel threads
              print_every_n = 20)
```

#binding all eta runs
```{r}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(wine_eta1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(wine_eta1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(wine_eta2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(wine_eta2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(wine_eta3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(wine_eta3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(wine_eta4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(wine_eta4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(wine_eta5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.001, nrow(wine_eta5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
eta_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
eta_data$eta <- as.factor(eta_data$eta)
```

# plotting multiple runs
```{r}
eta_plot <- ggplot(eta_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
eta_plot
```

## Confirming iiteration ammount with new eta
```{r}
set.seed(1984)

wine_it <- xgb.cv(data = wtrain, 
              
              nfold = 5, 
               
               eta = 0.005, 
              
               nrounds = 10000, 
               early_stopping_rounds = 50, 
               
               verbose = 1, 
               nthread = 5, 
               print_every_n = 20)
```

```{r}
set.seed(1984)

wine_it_light <- xgb.cv(data = wtrain, 
              
              nfold = 5, 
               
               eta = 0.01, 
              
               nrounds = 8000, 
               early_stopping_rounds = 50, 
               
               verbose = 1, 
               nthread = 5, 
               print_every_n = 20)
```
## 7500 minimum on new learning rate

## Secound try will changed with the prefered tuned variables.
```{r}
winebeast_1 <- xgboost(data = wtrain, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
               
              nrounds = 8500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 5, # Set number of parallel threads
              print_every_n = 40) 



winebeast_1_preds <- predict(winebeast_1, wtrain) 



winebeast_1_preds1 <- predict(winebeast_1, wtest)

rmse(actual, winebeast_1_preds1)


```
### RSME of 1.618199 COncerned of hw low the current outcome is. This model is the main concern of the group, before moving forward with and other modeling.

### Method to create new prediction without the price included
```{r}
non_price <-  wine_fixed[,-4]
```
# COnvert to numeric. 
```{r}
np_XG <- non_price

np_XG[,c(2,4:619)] <-  lapply(np_XG[,c(2,4:619)], as.numeric)
```


### New stratisize split for non price data frame.
```{r}
set.seed(1984)
nonpric_split <-  stratified(np_XG, c('Score'), .80,bothSets = TRUE)
np_train_strat <- nonpric_split[[1]]
np_test_strat <- nonpric_split[[2]]
```

### New XGboost groups for no price
```{r}
nptrain <- xgb.DMatrix(data = as.matrix(np_train_strat[,c(2,4:619)]), label = as.numeric(np_train_strat$Score))
# Create test matrix
nptest <- xgb.DMatrix(data = as.matrix(np_test_strat[, c(2, 4:619)]), label = as.numeric(np_test_strat$Score)) 
```
# Eunning the XGboost without the price variable.
```{r}
wine_noprice <- xgboost(data = nptrain, 
               
              eta = 0.01, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
               
              nrounds = 8500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 5, # Set number of parallel threads
              print_every_n = 40) 



wine_noprice_preds <- predict(wine_noprice, nptrain) 



wine_noprice_pred1 <- predict(wine_noprice, nptest)

rmse(np_test_strat$Score, wine_noprice_pred1)


```
 
## Top atteibutes for full XG
```{r}
# Extract importance
fullxg_mat <- xgb.importance(model = winebeast_1)
# Plot importance (top 10 variables)
xgb.plot.importance(fullxg_mat, top_n = 10)
```


## Top attrinutes for noprice XG
```{r}
# Extract importance
npxg_mat <- xgb.importance(model = wine_noprice)
# Plot importance (top 10 variables)
xgb.plot.importance(npxg_mat, top_n = 10)
```
## Graph of Full XG
```{r}
act_wine_full <- test_strat$Score

pred_wine_full <- winebeast_1_preds1


wine_full_data <- cbind.data.frame(act_wine_full, pred_wine_full)
wine_full_data$col <- log(abs(act_wine_full - pred_wine_full) + 1)
wine_full_plot <- ggplot(wine_full_data[act_wine_full > 60,], aes(x = act_wine_full, y = pred_wine_full, color = col)) + ## removing outlier score of 60 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score for Full XG",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nactual score")
wine_full_plot
```


## Graph of noPrice XG
```{r}
act_wine_np <- np_test_strat$Score

pred_wine_np <- wine_noprice_pred1


wine_np_data <- cbind.data.frame(act_wine_np, pred_wine_full)
wine_np_data$col <- log(abs(act_wine_np - pred_wine_full) + 1)
wine_np_plot <- ggplot(wine_np_data[act_wine_np > 60,], aes(x = act_wine_np, y = pred_wine_full, color = col)) + ## removing outlier score of 60 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score FOr No Price XG",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nactual score")
wine_np_plot

```


## SHAP Code Full XG 1 of 3
```{r}
XG_exp = buildExplainer(winebeast_1, wtrain, type = "regression", base_score = 0.5, trees_idx = NULL)
```
## SHAP Code FUll XG 2 of 3
```{r}
pred_full_breakdown = explainPredictions(winebeast_1, XG_exp, wtest) # Breakdown predictions
```
## SHAP Code FUll XG 3 of 3
```{r}
showWaterfall(winebeast_1, XG_exp, wtest, as.matrix(np_test_strat[, 2,4:220]) ,1441, type = "regression", threshold = 0.07)
```

## SHAP code noPrice XG 1 of 3
```{r}
npXG_exp = buildExplainer(wine_noprice, nptrain, type = "regression", base_score = 0.5, trees_idx = NULL)
```
## SHAP code noPrice 2 of 3
```{r}
pred_np_breakdown = explainPredictions(wine_noprice, npXG_exp, nptest)
```
## SHAP code noPrice 3 of 3
```{r}
showWaterfall(wine_noprice, npXG_exp, nptest, as.matrix(test_strat[, 2,4:220]) ,1441, type = "regression", threshold = 0.07)
```



```{r}
# Martin graph code 
plot_data <- cbind.data.frame(actual, predicted)
plot_data$col <- log(abs(actual - predicted) + 1)
g_1 <- ggplot(plot_data[actual > 60,], aes(x = actual, y = predicted, color = col)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nactual score")
g_1
```

```{r}
# Tuning Loop - also Martin's code 
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
rmse_vec <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(111111)
  bst_tune <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = cv_params$max_depth[i], # Set max depth
              min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no im
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20) # Prints out result every 20th iteration

  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
}
```

```{r}
# More Martin ggplot code 
# Join results in dataset
res_db <- cbind.data.frame(wcv_params, rmse_vec)
names(res_db)[3:4] <- c("rmse") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print RMSE heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$rmse), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "RMSE") # Set labels
g_2 # Generate plot
```

```{r}
# SHAP results 
# look at Tuesday's code 
```

# TODO:
- Impute NAs
- Check RandomForest
- Tune xgBoost
- look at SHAP Results / feature importance
- Draw insight from SHAP
- RMSE plot
