---
title: "Bordeaux Wine Reviewer"
author: "Shivam Patel, Ethan Gruis, Ben Siglow"
date: "10/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages

```{r Load Packages/Install}
##install.packages("SHAPforxgboost")
#install_github("AppliedDataSciencePartners/xgboostExplainer")
library(devtools)
library(xgboostExplainer)
library(tidyverse)
library(ggdark)
library(rattle)	
library(RColorBrewer)
library(randomForest)
library(caTools)
library(caret)
library(rpart)
library(splitstackshape)
library(xgboost)
library(Metrics)
library(pROC)
library(SHAPforxgboost)
library(xgboostExplainer)
source("a_insights_shap_functions.r")
```

# Data Cleaning and Exploration
## Loading Data
```{r Load Data}
link <- 'https://www.dropbox.com/s/mjj5x2n2wfjxqs9/BordeauxWines.csv?dl=1'
wine_data <- read_csv(link, locale = readr::locale(encoding = "latin1"))

# Commented out formatting purposes, prior data review/exploration
# summary(wine_data)
# str(wine_data)
```

## Mutate our variables into factors & remove variables with constant values

```{r Variable Mutation}
wine_cols <- c(5:989)
wine_data[,wine_cols] <- lapply(wine_data[,wine_cols], factor)

# ISSUE: when factorized, wine sometimes has columns with only 1 factor, this selects only columns with multiple factors and drops the rest
wine_fixed <- wine_data[, sapply(wine_data, function(col) length(unique(col))) > 1]
```

## Remove dollar signs from Price.
```{r Cleaning Data-Remove NAs}
# Fix Price Variable
wine_fixed$Price <- str_replace(wine_fixed$Price, "\\$", "")

wine_fixed$Price <- as.numeric(wine_fixed$Price)

# Creating no price dataframe without dropping NAs
noprice_wine_fixed <- wine_fixed

wine_fixed <- drop_na(wine_fixed)

# Fix ColNames
colnames(wine_fixed) <- make.names(names(wine_fixed))

# For some reason there are two WELL DONE columns, so lets change the name of one of them?
colnames(wine_fixed)[489] <- 'WELL.DONE.2'
```

# Visualizations
```{r Data Exploration-Visualizations}

# Score v Price
price_score_plot <- ggplot(wine_fixed, aes(x = Price, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

price_score_plot

# Score v Year
score_year_plot <- ggplot(wine_fixed, aes(x = Year, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

score_year_plot

# Score Histogram
hist_score_plot <- ggplot(wine_fixed, aes(x = Score)) + 
  geom_histogram(binwidth = 1, color="black", fill="white") +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  xlim(70, 100) +
  dark_theme_gray()

hist_score_plot
```

# Linear regressions
```{r LM 1 Score ~ Price}
# LM 1 = Score predicted by Price
lm_1 <- lm(Score ~ Price, data = wine_fixed)

summary(lm_1)
```

```{r LM 2 Score ~ Year}
# LM 2 = Score predicted by Year
lm_2 <- lm(Score ~ Year, data = wine_fixed)

summary(lm_2)
```

# Decision Tree

## Stratified split to the train/test
```{r Splitting via Stratified}
set.seed(1984)

wine_no_name <- wine_fixed[2:620]

split <-  stratified(wine_no_name, c('Score'), .80,bothSets = TRUE)
train_strat <- split[[1]]
test_strat <- split[[2]]
```

## Run Decision Tree
```{r Decision Tree Full}
tree_1 <- rpart(Score ~.,
                data = train_strat)

plotcp(tree_1)
summary(tree_1)
```

## Prediction of Tree

```{r Decision Tree - Prediction}
wine_pred_no_name <- predict(tree_1, test_strat)

actual <- test_strat$Score
predicted <- wine_pred_no_name
rmse(actual, predicted)
```

# Random Forest

## Random Forest Model Results 

```{r Random Forest: Martin Barron}
load("./random_forest_model.rda")
rf_mod <- fit

print(rf_mod)

print(sqrt(rf_mod$mse[100]))
```

# XGBoost

## Created XG frame and used lapply to convert to numeric
```{r Numeric Conv. for XGBoost}
wine_XG <- wine_fixed

wine_XG[,c(2,4:620)] <-  lapply(wine_XG[,c(2,4:620)], as.numeric)
```


## Re-stratified split on new set of numeric data
```{r Re-Stratified Split}
set.seed(1984)

split <-  stratified(wine_XG, c('Score'), .80,bothSets = TRUE)
train_strat <- split[[1]]
test_strat <- split[[2]]
```

## First XG Run
```{r XGBoost Convert - DMatrix}
wtrain <- xgb.DMatrix(data = as.matrix(train_strat[,c(2,4:620)]), label = as.numeric(train_strat$Score))
# Create test matrix
wtest <- xgb.DMatrix(data = as.matrix(test_strat[, c(2, 4:620)]), label = as.numeric(test_strat$Score)) 
```


## Training Basic XG
```{r Basic XG, eval = FALSE}
set.seed(1984)
winebeast <- xgboost(data = wtrain, 
               eta = .3,
               nrounds = 1000, 
               nthread = 11, 
               verbose = 1, 
               print_every_n = 20)
save(winebeast, file = "./RData/winebeast.RData")
```

## Check Accuracy
```{r Accuray of XGBoost - RMSE}
load("./RData/winebeast.RData")

wine_preds_1 <- predict(winebeast, wtest)

actual <- test_strat$Score # Usable for all but no price XGBoost
predicted <- wine_preds_1
rmse(actual, predicted)
```

## Plotting results to first XG-boost.
```{r XGBoost Plotting}
# Martin Barron Graph Code

plot_data <- cbind.data.frame(actual, predicted)

plot_data$col <- log(abs(actual - predicted) + 1)

winebeast_plot <- ggplot(plot_data[actual > 60,], aes(x = actual, y = predicted, color = col)) + # removing outlier score of 60 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nactual score")

winebeast_plot
```

# Primary Tuning of XGBoost

## Check number of itterations in model
```{r Iteration Check for eta .1, eval = FALSE}
set.seed(1984)
wine_amount <- xgb.cv(data = wtrain, 
              nfold = 5, 
               eta = 0.1, 
               nrounds = 2000, 
               early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 11, 
               print_every_n = 20) 
```

### Best itteration count is 756, 1000 will be a realistic iterations number for final XGBoost.

## Heat Map Loop
```{r Heat Map Loop, eval = FALSE}
# Tuning Loop - Martin Barron contribution. 
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
wcv_params <- expand.grid(max_depth_vals, min_child_weight)
names(wcv_params) <- c("max_depth", "min_child_weight")
# Create results vector
rmse_vec <- rep(NA, nrow(wcv_params)) 
# Loop through results
for(i in 1:nrow(wcv_params)){
  set.seed(1984)
  bst_tune <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = wcv_params$max_depth[i], # Set max depth
              min_child_weight = wcv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no im
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 20) # Prints out result every 20th iteration

  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
}
```

## Heat Plot of Tuning
```{r Heat Map Plotting Run, eval = FALSE}
# Martin Baron ggplot code
# Join results in dataset
res_db <- cbind.data.frame(wcv_params, rmse_vec)
names(res_db)[3] <- c("rmse") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print RMSE heatmap
wine_heat <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$rmse), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "RMSE") # Set labels

save(wine_heat, file = './RData/wine_heat.RData')
```


```{r Heat Map Plotting}
load('./RData/wine_heat.RData')

wine_heat # Generate plot
```
### Max Depth Tuning Note

After additional examination of this heat plot, we had decided to change the max.depth setting from 15 to 7 in our new runs. However, we are not going to be re-running SHAP or the explainer at the end of this document. Any differences in RMSE or results will be highlighted in each section. Because of this change, we also will run through multiple etas using the new max.depth in order to ensure the best results.

## Combining Multiple etas for Optimal eta
### .1 eta Run
```{r .1 eta run, eval = FALSE}
set.seed(1984)
wine_eta1 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 20) # Set evaluation metric to use

```

### .05 eta Run
```{r .05 eta Run, eval = FALSE}
set.seed(1984)
wine_eta2 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 20)
```

### .01 eta Run
```{r .01 eta Run, eval = FALSE}
set.seed(1984)
wine_eta3 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 20)
```

### .005 eta Run
```{r .005 eta Run, eval = FALSE}
set.seed(1984)
wine_eta4 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 20)
```

### .001 eta Run
```{r .001 eta Run, eval = FALSE}
set.seed(1984)
wine_eta5 <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.001, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 20)
```

## Binding All etas for Graph
```{r  Binding etas, eval = FALSE}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(wine_eta1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(wine_eta1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(wine_eta2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(wine_eta2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(wine_eta3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(wine_eta3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(wine_eta4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(wine_eta4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(wine_eta5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.001, nrow(wine_eta5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
eta_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
eta_data$eta <- as.factor(eta_data$eta)
```

## Plotting eta Runs
```{r Plotting eta Runs - Pre, eval = FALSE}
eta_plot_NEW <- ggplot(eta_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees (Max Depth = 7)",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels

save(eta_plot_NEW, file = './RData/eta_plot_NEW.RData')
```


```{r Plotting eta Runs}
load('./RData/eta_plot.RData')
load('./RData/eta_plot_NEW.RData')

eta_plot

eta_plot_NEW
```

Seeing the new plot with Max Depth set to 7, our decision for the learning rate maintains and we will stick with 0.01.

## Iteration Optimization
```{r Iteration Optimization, eval = FALSE}
# Optimization for eta at .01
set.seed(1984)

wine_it_light <- xgb.cv(data = wtrain, 
              nfold = 5, 
               eta = 0.01, 
               nrounds = 8000, 
               early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 11, 
               print_every_n = 20)
```
### 7500 minimum on new learning rate

# Final XG Models and Visuals

## Optimal XGBoost with all Tuning Aspects
```{r Optimal XGBoost Model - With Price (Run), eval = FALSE}
# We initially had the max.depth set to 15 and 
winebeast_1 <- xgboost(data = wtrain, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
               
              nrounds = 8500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 40)

save(winebeast_1, file = "./RData/winebeast_1.RData")
```


```{r Optimal XGBoost Model - With Price}
load("./RData/winebeast_1.RData")

winebeast_1_preds <- predict(winebeast_1, wtrain) 

winebeast_1_preds1 <- predict(winebeast_1, wtest)

rmse(actual, winebeast_1_preds1)
```
### Original RMSE: 1.672438 (Max Depth = 15)
### New RMSE: 1.605214 (Max Depth = 7)

## XGBoost with no Price
### Create No Price Data Frame
```{r non_price Data Frame}
non_price <-  noprice_wine_fixed[,-4]

np_XG <- non_price

np_XG[,c(2,4:619)] <-  lapply(np_XG[,c(2,4:619)], as.numeric)
```

### New Stratisize Split for Non-Price Data Frame
```{r Stratisized Split for NP DF}
set.seed(1984)

nonpric_split <-  stratified(np_XG, c('Score'), .80,bothSets = TRUE)

np_train_strat <- nonpric_split[[1]]
np_test_strat <- nonpric_split[[2]]
```

### New XGboost Groups for No Price
```{r New XGBoost Groups- No Price}
# Create train matrix
nptrain <- xgb.DMatrix(data = as.matrix(np_train_strat[,c(2,4:619)]), label = as.numeric(np_train_strat$Score))
# Create test matrix
nptest <- xgb.DMatrix(data = as.matrix(np_test_strat[, c(2, 4:619)]), label = as.numeric(np_test_strat$Score)) 
```

## Running the XGboost Without the Price Variable.
```{r XGBoost Run - No Price (Run), eval = FALSE}
wine_noprice <- xgboost(data = nptrain, 
              eta = 0.01, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 15, # Set minimum number of samples in node to split
              nrounds = 8500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
              verbose = 1, # 1 - Prints out fit
              nthread = 11, # Set number of parallel threads
              print_every_n = 40) 

save(wine_noprice, file = "./RData/wine_noprice.RData")
```


```{r XGBoost Run - No Price}
load("./RData/wine_noprice.RData")

wine_noprice_preds <- predict(wine_noprice, nptrain) 

wine_noprice_pred1 <- predict(wine_noprice, nptest)

rmse(np_test_strat$Score, wine_noprice_pred1)
```
# Original RMSE: 1.775378 (Max Depth = 15)
# New RMSE: 1.728415 (Max Depth = 7)
 
# Results for XGBoost
```{r Results for XGBoost - With Price}
# Extract importance
fullxg_mat <- xgb.importance(model = winebeast_1)
# Plot importance (top 10 variables)
xgb.plot.importance(fullxg_mat, top_n = 10)
```


## Top attributes for No Price XG
```{r Attributes - No Price XG}
# Extract importance
npxg_mat <- xgb.importance(model = wine_noprice)
# Plot importance (top 10 variables)
xgb.plot.importance(npxg_mat, top_n = 10)
```

## Graph of Full XG
```{r Graph of Full XG}
act_wine_full <- test_strat$Score

pred_wine_full <- winebeast_1_preds1


wine_full_data <- cbind.data.frame(act_wine_full, pred_wine_full)
wine_full_data$col <- log(abs(act_wine_full - pred_wine_full) + 1)
wine_full_plot <- ggplot(wine_full_data[act_wine_full > 60,], aes(x = act_wine_full, y = pred_wine_full, color = col)) + ## removing outlier score of 60 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score for Full XG",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nActual Score")
wine_full_plot
```


## Graph of No Price XG
```{r Graph of No Price XG}
act_wine_np <- np_test_strat$Score

pred_wine_np <- wine_noprice_pred1
#wine_np_data <- cbind.data.frame(act_wine_np, pred_wine_full)
wine_np_data <- cbind.data.frame(act_wine_np, pred_wine_np)
#wine_np_data$col <- log(abs(act_wine_np - pred_wine_full) + 1)
wine_np_data$col <- log(abs(act_wine_np - pred_wine_np) + 1)
wine_np_plot <- ggplot(wine_np_data[act_wine_np > 60,], aes(x = act_wine_np, y = pred_wine_np, color = col)) + ## removing outlier score of 60 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score for No Price XG",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nActual Score")
wine_np_plot
```

## SHAP and XGBoost Explainer

This section was ran prior to the change of Max Depth, all of these graphs are using the model's that had the Max Depth of 15.

For the sake of time we chose not to re-run this section.

## SHAP Code Full XG 1 of 3
```{r Waterfall Price 1/3 (Run), eval = FALSE}
XG_exp = buildExplainer(winebeast_1, wtrain, type = "regression", base_score = 0.5, trees_idx = NULL)

save(XG_exp, file = "XG_exp_shap.RData")
```

```{r Waterfall Price 1/3}
load('./RData/XG_exp_shap.RData')
```

## SHAP Code FUll XG 2 of 3
```{r Waterfall Price 2/3 (Run), eval = FALSE}
pred_full_breakdown = explainPredictions(winebeast_1, XG_exp, wtest) # Breakdown predictions

save(pred_full_breakdown, file = "SHAP_full_breakdown_fullXG.RData")
```

```{r Waterfall Price 2/3, eval = FALSE}
load('./RData/SHAP_full_breakdown_fullXG.RData')

pred_full_breakdown
```

## SHAP Code FUll XG 3 of 3
```{r Waterfall Price 3/3 (Run), eval = FALSE}
waterfall_shap_fullXG = showWaterfall(winebeast_1, XG_exp, wtrain, as.matrix(test_strat[, c(2, 4:620)]) ,1441, type = "regression", threshold = 0.07)

save(waterfall_shap_fullXG, file = "waterfall_shap_fullXG.RData")
```

```{r Waterfall Price 3/3}
load('./RData/waterfall_shap_fullXG.RData')

waterfall_shap_fullXG
```

## SHAP code noPrice XG 1 of 3
```{r Waterfall No Price 1/3 (Run), eval = FALSE}
npXG_exp = buildExplainer(wine_noprice, nptrain, type = "regression", base_score = 0.5, trees_idx = NULL)

save(npXG_exp, file = "XG_exp_shap_NP.RData")
```

```{r Waterfall No Price 1/3}
load('./RData/shap_np_xg_exp.RData')
```


## SHAP code noPrice 2 of 3
```{r Waterfall No Price 2/3 (Run), eval = FALSE}
pred_np_breakdown = explainPredictions(wine_noprice, npXG_exp, nptest)

save(pred_np_breakdown, file = "SHAP_np_breakdown.RData")
```

```{r Waterfall No Price 2/3, eval = FALSE}
load('./RData/SHAP_np_breakdown.RData')

pred_np_breakdown
```
## SHAP code noPrice 3 of 3
```{r Waterfall No Price 3/3 (Run), eval = FALSE}
waterfall_shap_np = showWaterfall(wine_noprice, npXG_exp, nptrain, as.matrix(np_test_strat[, c(2, 4:619)]) ,1441, type = "regression", threshold = 0.07)

save(waterfall_shap_np, file = "waterfall_shap_np.RData")
```


```{r Waterfall No Price 3/3}
load('./RData/waterfall_shap_np.RData')

waterfall_shap_np
```

# SHAP Scores
## For the price model

```{r Shap Results Price (Run), eval = FALSE}
shap_result <- shap.score.rank(xgb_model = winebeast_1,
                               X_train = as.matrix(train_strat[,c(2,4:620)]),
                               shap_approx = F)

save(shap_result, file = "shap_results.RData")
```


```{r Shap Results Price}
load('./RData/shap_results.RData')

var_importance(shap_result, top_n=10)
```

```{r Shap Long Price (Run), eval = FALSE}
shap_long = shap.prep(shap = shap_result,
                      X_train = as.matrix(train_strat[,c(2,4:620)]),
                      top_n = 10)

save(shap_long, file = "shap_long.RData")
```


```{r Shap Long Price}
load('./RData/shap_long.RData')

plot.shap.summary(data_long = shap_long)
```

## For the no-price model
```{r Shap Results No Price (Run), eval = FALSE}
shap_result_np <- shap.score.rank(xgb_model = wine_noprice,
                                  X_train = as.matrix(np_train_strat[,c(2,4:619)]),
                                  shap_approx = F)

save(shap_result_np, file = "shap_results_np.RData")
```


```{r Shap Results No Price}
load('./RData/shap_results_np.RData')

var_importance(shap_result_np, top_n = 10)
```

```{r Shap Long No Price (Run), eval = FALSE}
shap_long_np = shap.prep(shap = shap_result_np,
                         X_train = as.matrix(np_train_strat[,c(2,4:619)]),
                         top_n = 10)

save(shap_long_np, file = "shap_long_np.RData")
```


```{r Shap Long No Price}
load("./RData/shap_long_np.RData")

plot.shap.summary(data_long = shap_long_np)
```
