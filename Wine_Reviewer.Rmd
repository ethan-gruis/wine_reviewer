---
title: "ML_Problem_Proposal_Wine"
author: "Shivam Patel, Ethan Gruis, Ben Siglow"
date: "10/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggdark)
```


Load in the data

```{r}
link <- 'https://www.dropbox.com/s/mjj5x2n2wfjxqs9/BordeauxWines.csv?dl=1'
wine_data <- read_csv(link, locale = readr::locale(encoding = "latin1"))
```

```{r}
# summary(wine_data)
# str(wine_data)
```

# Mutate our variables into factors

```{r}
wine_cols <- c(5:989)
wine_data[,wine_cols] <- lapply(wine_data[,wine_cols], factor)
#wine_data[,wine_cols] <- lapply(wine_data[,wine_cols], factor, level = c(0, 1))

# ISSUE: when factorized, wine sometimes has columns with only 1 factor, this selects only columns with multiple factors and drops the rest
wine_fixed <- wine_data[, sapply(wine_data, function(col) length(unique(col))) > 1]

# str(wine_fixed, list.len=ncol(wine_fixed))
```

Remove dollar signs from Price.

```{r}
wine_fixed$Price <- str_replace(wine_fixed$Price, "\\$", "")

wine_fixed$Price <- as.numeric(wine_fixed$Price)

colnames(wine_fixed) <- make.names(names(wine_fixed))
colnames(wine_fixed)[489] <- 'WELL.DONE.2'
# what does well done mean in terms of wine???
# look at price - some are in $/ML
```

# Visualizations

```{r}
price_score_plot <- ggplot(wine_fixed, aes(x = Price, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

price_score_plot

score_year_plot <- ggplot(wine_fixed, aes(x = Year, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

score_year_plot

hist_score_plot <- ggplot(wine_fixed, aes(x = Score)) + 
  geom_histogram(binwidth = 1, color="black", fill="white") +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  xlim(70, 100) +
  dark_theme_gray()

hist_score_plot
```

# Linear regressions

```{r}
lm_1 <- lm(Score ~ Price, data = wine_fixed)

summary(lm_1)
```

```{r}
lm_2 <- lm(Score ~ Year, data = wine_fixed)

summary(lm_2)
```


```{r}
# Test glm with factors
# Doesnt work obviously
# log_fit_1 <- glm(Score ~., # Set formula
#              family=gaussian(link='identity'), # Set logistic regression
#              data= wine_fixed) # Set dataset
# summary(log_fit_1)
```

```{r}
summary(log_fit_1)
```

```{r}
library(rpart)	

tree_1 <- rpart(Score ~., # Set tree formula
                data = wine_fixed)

plotcp(tree_1) # Plot cp
```

```{r}
tree_2 <- tree_1 <- rpart(Score ~., # Set tree formula
                          data = wine_fixed, # Set data
                          control = rpart.control(cp = 0.017)) # Set parameters
```

```{r}
library(rattle)					# Fancy tree plot
library(RColorBrewer)				# Color selection for fancy tree plot

summary(tree_2)

fancyRpartPlot(tree_2)
```
## Code Borrowed From Random Forest In Parallel GitHub:
https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md

### NOTE: BE CAREFUL RUNNING ANYTHING BELOW THIS LINE

```{r}
library(randomForest)
library(caTools)
library(caret)
set.seed(612)

inTraining <- createDataPartition(wine_fixed$Score, p = .75, list=FALSE)
training <- wine_fixed[inTraining,]
testing <- wine_fixed[-inTraining,]
```

```{r}
set.seed(258506) # Set random number generator seed for reproducability
# Use random forest to do bagging
bag_mod <- randomForest(Score ~., # Set tree formula
                data = training[,3:620], # Set dataset 
                ntree = 200,
                na.action=na.exclude,
                do.trace = TRUE) # Set number of trees to use
bag_mod # View model
```


```{r}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```

```{r}
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
```

Solution for NA problem: 
https://stackoverflow.com/questions/38250440/error-in-na-fail-default-missing-values-in-object-but-no-missing-values
```{r}
fit <- randomForest(Score ~., # Set tree formula
                data = training[,3:620], # Set dataset 
                ntree = 100,
                na.action=na.exclude,
                do.trace = TRUE)
```

```{r}
stopCluster(cluster)
registerDoSEQ()
```

Notes:
- Bem create split better
- Shiv do shiv things
- XGBoost: start playing with XGBoost :)





=======
---
title: "ML_Problem_Proposal_Wine"
author: "Shivam Patel, Ethan Gruis, Ben Siglow"
date: "10/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggdark)
```


Load in the data

```{r}
link <- 'https://www.dropbox.com/s/mjj5x2n2wfjxqs9/BordeauxWines.csv?dl=1'
wine_data <- read_csv(link, locale = readr::locale(encoding = "latin1"))

# summary(wine_data)
# str(wine_data)
```



# Mutate our variables into factors & Remove variables with constant values

```{r}
wine_cols <- c(5:989)
wine_data[,wine_cols] <- lapply(wine_data[,wine_cols], factor)

# ISSUE: when factorized, wine sometimes has columns with only 1 factor, this selects only columns with multiple factors and drops the rest
wine_fixed <- wine_data[, sapply(wine_data, function(col) length(unique(col))) > 1]
```

# Remove dollar signs from Price.

```{r}
# Fix Price Variable
wine_fixed$Price <- str_replace(wine_fixed$Price, "\\$", "")

wine_fixed$Price <- as.numeric(wine_fixed$Price)

# Fix ColNames
colnames(wine_fixed) <- make.names(names(wine_fixed))

# For some reason there are two WELL DONE columns, so lets change the name of one of them?
colnames(wine_fixed)[489] <- 'WELL.DONE.2'

# Notes:
# what does well done mean in terms of wine???
# look at price - some are in $/ML
```

# Visualizations

```{r}
price_score_plot <- ggplot(wine_fixed, aes(x = Price, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

price_score_plot

score_year_plot <- ggplot(wine_fixed, aes(x = Year, y = Score)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  dark_theme_gray()

score_year_plot

hist_score_plot <- ggplot(wine_fixed, aes(x = Score)) + 
  geom_histogram(binwidth = 1, color="black", fill="white") +
  theme(panel.grid.major = element_blank(), # Turn of the background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank()) +
  xlim(70, 100) +
  dark_theme_gray()

hist_score_plot
```

# Linear regressions

```{r}
lm_1 <- lm(Score ~ Price, data = wine_fixed)

summary(lm_1)
```

```{r}
lm_2 <- lm(Score ~ Year, data = wine_fixed)

summary(lm_2)
```

```{r}
##summary(log_fit_1)
#### What is this?
```

```{r}
library(rpart)	

tree_1 <- rpart(Score ~., # Set tree formula
                data = wine_fixed)

plotcp(tree_1) # Plot cp
```

```{r}
tree_2 <- tree_1 <- rpart(Score ~., # Set tree formula
                          data = wine_fixed, # Set data
                          control = rpart.control(cp = 0.017)) # Set parameters
```

```{r}
library(rattle)					# Fancy tree plot
library(RColorBrewer)				# Color selection for fancy tree plot

summary(tree_2)

fancyRpartPlot(tree_2)
```

## Code Borrowed From Random Forest In Parallel GitHub:
https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md

```{r}
library(randomForest)
library(caTools)
```
### NOTE: BE CAREFUL RUNNING ANYTHING BELOW THIS LINE

```{r}
library(caret)
set.seed(612)

inTraining <- createDataPartition(wine_fixed$Score, p = .75, list=FALSE)
training <- wine_fixed[inTraining,]
testing <- wine_fixed[-inTraining,]
```

##Strat split to the train/test. BS
```{r}
library(splitstackshape)
set.seed(1984)
split <-  stratified(wine_fixed, c('Score'), .80,bothSets = TRUE)
train_strat <- split[[1]]
test_strat <- split[[2]]
```

# Deprecated Model Graveyard
```{r}
# Bag mod no good 
# set.seed(258506) # Set random number generator seed for reproducability
# # Use random forest to do bagging
# bag_mod <- randomForest(Score ~., # Set tree formula
#                 data = training[,3:620], # Set dataset 
#                 ntree = 200,
#                 na.action=na.exclude,
#                 do.trace = TRUE) # Set number of trees to use
# bag_mod # View model
```

```{r}
# Random Forest no good
# fit <- randomForest(Score ~., # Set tree formula
#                 data = training[,3:620], # Set dataset 
#                 ntree = 100,
#                 do.trace = TRUE,
#                 na.action=na.exclude)
```


### Commenting out parallel processing for knitting and MArtin meeting wil return to normal for possible additional models later. BS.

<!-- # Lets try parallel: -->
<!-- ```{r} -->
<!-- library(parallel) -->
<!-- library(doParallel) -->
<!-- cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS -->
<!-- registerDoParallel(cluster) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- fitControl <- trainControl(method = "cv", -->
<!--                            number = 5, -->
<!--                            allowParallel = TRUE) -->
<!-- ``` -->

<!-- Solution for NA problem:  -->
<!-- https://stackoverflow.com/questions/38250440/error-in-na-fail-default-missing-values-in-object-but-no-missing-values -->

<!-- ```{r} -->
<!-- stopCluster(cluster) -->
<!-- registerDoSEQ() -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(xgboost) -->
<!-- ``` -->
<!-- ## Previous try not used in this current XGboost model. -->
<!-- ```{r} -->
<!-- winetrain <- xgb.DMatrix(data = as.matrix()) -->
<!-- ``` -->

Notes:
- XGBoost: start playing with XGBoost :)

## XGboos apparently needs us to have our cols as numeric not factors.
## Created XG frame and used lapply to convert to numeric. BS
```{r}
library(xgboost)
wine_XG <- wine_fixed

wine_XG[,2:620] <-  lapply(wine_XG[2,4:620], as.numeric)
```


## Re Strat split on new set of numeric Data. BS
```{r}
set.seed(1984)
split <-  stratified(wine_XG, c('Score'), .80,bothSets = TRUE)
train_strat <- split[[1]]
test_strat <- split[[2]]

```

- 
### First XG Run. BS
### Converting to xgb magic. BS
```{r}
library(xgboost)
wtrain <- xgb.DMatrix(data = as.matrix(train_strat[, 2:620]), label = as.numeric(train_strat$Score))
# Create test matrix
wtest <- xgb.DMatrix(data = as.matrix(test_strat[, 2:620]), label = as.numeric(test_strat$Score)) 
```


## Training basic XG. BS
```{r}
set.seed(1984)
winebeast <- xgboost(data = wtrain, 
               
               nrounds = 1000, 
               
               verbose = 1, 
                print_every_n = 20, )

```

## Testing basic XG. BS
```{r}
wine_preds <- predict(winebeast, wtrain) 
# Join predictions and actual
wine_dat <- cbind.data.frame(wine_preds , train_strat$Score)
names(wine_dat) <- c("predictions", "response")


wine_preds_1 <- predict(winebeast, wtest)


```
## Check Accuracy. BS
```{r}
##install.packages("Metrics")
## Based on this remedeal mean square we murdered this. 
library(Metrics)
actual <- test_strat$Score
predicted <- wine_preds_1
rmse(actual, predicted)
```
## Resulting RSME value of .008172747. This is great, but we plan to look at some minor tuning to determine better values. 

```{r}
library(pROC)
wroc1 = roc(test_strat$Score, wine_preds_1)

plot.roc(wroc1, print.auc = TRUE, col = "red", print.auc.col = "red")
```
### Concernes on if this ROC curve is at all correct. I am not sure we are even able to determine AUC on this type of model. BS.



## Secound try
```{r}
winebeast_1 <- xgboost(data = wtrain, 
              
        
               
              eta = 0.03, 
               
              nrounds = 1000, 
              early_stopping_rounds = 20,  
               
              verbose = 1, 
              nthread = 1, 
              print_every_n = 20) 



winebeast_1_preds <- predict(winebeast_1, wtrain) 



winebeast_1_preds1 <- predict(winebeast_1, wtest)

rmse(actual, winebeast_1_preds1)


```
### RSME of .0001268033. COncerned of hw low the current outcome is. This model is the main concern of the group, before moving forward with and other modeling. 
```{r}
wroc2 <-  roc(test_strat$Score, winebeast_1_preds1)

plot.roc(wroc2, print.auc = TRUE, col = "red", print.auc.col = "red")
```
### Confused on the levels warning message, however I assume this is, because of how roc cureves work and they can only be used within the classification model. Apologies for model measures. BS. 


```{r}
# Martin graph code 
plot_data <- cbind.data.frame(actual, predicted)
plot_data$col <- log(abs(actual - predicted) + 1)
g_1 <- ggplot(plot_data[actual > 60,], aes(x = actual, y = predicted, color = col)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  scale_color_gradient(low = "blue", high = "red") +
  dark_theme_bw() + 
  theme(axis.line = element_line(colour = "white"), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank(),
        plot.subtitle = element_text(face = "italic")) +
  labs(x = "Actual Wine Score", y = "Predicted Wine Score",
       title = "Predicted vs Actual Wine Score",
       subtitle = "XGBoost Model 1",
       color = "Deviation from\nactual score")
g_1
```

```{r}
# Tuning Loop - also Martin's code 
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
rmse_vec <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(111111)
  bst_tune <- xgb.cv(data = wtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = cv_params$max_depth[i], # Set max depth
              min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no im
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20) # Prints out result every 20th iteration

  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
}
```

```{r}
# More Martin ggplot code 
# Join results in dataset
res_db <- cbind.data.frame(cv_params, rmse_vec)
names(res_db)[3:4] <- c("rmse") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print RMSE heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$rmse), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "RMSE") # Set labels
g_2 # Generate plot
```

```{r}
# SHAP results 
# look at Tuesday's code 
```

# TODO:
- Imput NAs
- Check RandomForest
- Tune xgBoost
- look at SHAP Results / feature importance
- Draw insight from SHAP
- RMSE plot
